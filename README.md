# ReBalance-Tackling-Class-Imbalance-in-ML
Class imbalance is a critical challenge in many real-world classification tasks, particularly in domains like fraud detection, where the minority class represents only a small fraction of the total data. This study explores the effectiveness of various resampling techniques—SMOTE, ADASYN, BorderlineSMOTE, SMOTETomek, and SMOTEENN—on improving classification performance across multiple machine learning models, including Logistic Regression, Random Forest, XGBoost, and K-Nearest Neighbors (KNN). Using a publicly available credit card fraud detection dataset characterized by a highly skewed class distribution, we evaluate each model-resampling combination based on key metrics such as F1-score, AUC, precision, and recall, with a special emphasis on the minority class. The experiments reveal that certain resampling strategies, particularly hybrid techniques like SMOTETomek and SMOTEENN, can significantly enhance minority class detection while maintaining overall model stability. These insights are valuable for practitioners seeking to develop robust predictive models in imbalanced classification settings.
