# ReBalance-Tackling-Class-Imbalance-in-ML
This project analyzes the impact of various resampling techniques—SMOTE, ADASYN, BorderlineSMOTE, SMOTETomek, and SMOTEENN—on the performance of machine learning models like Logistic Regression, Random Forest, XGBoost, and KNN. Using a credit card fraud detection dataset, we evaluate models using F1-score, AUC, and precision-recall metrics.
